---
title: "Data Mining HW2 Final Code"
author: "Blue Team 16"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: TRUE
    df_print: paged
---

```{r,include = F}
library(rpart)
library(tidyverse)
library(rpart.plot)
library(datasets)
library(arules)
library(arulesViz)
library(ggplot2)
library(dplyr)
library(rpart)
library(rpart.plot)
library(TH.data)
library(ISLR2)
library(lattice)
library(stats)
library(rattle)
library(RColorBrewer)
library(caret)
library(ROCR)
library(tidyverse)  
library(cluster)  
library(factoextra) 
library(gridExtra)
library(NbClust)
library(dendextend)
library(class)
library(ClustOfVar)
library(gmodels)
library(DescTools)
library(ROCit)
library(survival)
library(MASS)
library(kableExtra)
library(partykit)
library(dbscan)
library(AmesHousing)
library(conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("prune","rpart")
library(rstudioapi)
library(reticulate)
```


# Data Reading
```{r}
setwd(dirname(getActiveDocumentContext()$path))

# Reading in data
trainingBin <- read.csv("insurance_t_bin.csv") 
training <- read.csv("insurance_t.csv") 

validationBin <- read.csv("insurance_v_bin.csv") 
validation <- read.csv("insurance_v.csv") 

# Fixing Separations and NAs
trainingBin <- trainingBin %>% mutate(across(everything(), ~ as.character(.x))) %>% 
  mutate(across(everything(), ~ replace_na(.x,"M"))) %>% 
  mutate(across(everything(), ~ as.factor(.x)))

validationBin <- validationBin %>% mutate(across(everything(), ~ as.character(.x))) %>% 
  mutate(across(everything(), ~ replace_na(.x,"M"))) %>% 
  mutate(across(everything(), ~ as.factor(.x)))
```

# Old Logistic Regression Model
```{r}
finalModel <- glm(INS ~ NSF + MTG + INV + ILSBAL_BIN + IRA + DDA + TELLER_BIN + CC + ATMAMT_BIN + CHECKS_BIN + MMBAL_BIN + CDBAL_BIN + DDABAL_BIN + SAVBAL_BIN + DDA:IRA,family = binomial(link = "logit"),data = trainingBin)
```

# Decision Tree Models
```{r}
# Making a large tree to prune later. The values I selected are what I came to after playing around with various models.

# Only LR variables
lrTree <- rpart(INS ~ NSF + MTG + INV + ILSBAL + IRA + DDA + TELLER + CC + ATMAMT + CHECKS + MMBAL + CDBAL + DDABAL + SAVBAL, data=training, method='class',parms = list(split="gini"),
                    control = rpart.control(minsplit = 30, cp = .001, maxdepth = 6))
# All variables
bigTree <- rpart(INS ~ ., data=training, method='class',parms = list(split="gini"),
                       control = rpart.control(minsplit = 30, cp = .001, maxdepth = 6))
```


## Pruning

### Subset Variable Model
```{r}
printcp(lrTree)
```
Only want to include first 6 layers based on oneSE
```{r}
lrTree <- prune(lrTree,cp=0.0049692)
```

### Full Variable Model Tree

```{r}
printcp(bigTree)
```

Only want first 10 layers

```{r}
bigTree <- prune(bigTree,cp=0.0020562)
```

# Visualizing 

## Subset Variable Model
```{r,echo = F}
rpart.plot(lrTree,roundint = F)
```

## Full Variable Model
```{r,echo = F}
rpart.plot(bigTree)
```


# Accuracy scores

## Predictions and Fitted Values
```{r}
probLRTree <- predict(lrTree,validation,type = "prob")
probBigTree <- predict(bigTree,validation,type = "prob")

predLRTree <- predict(lrTree,validation,type = "class")
predBigTree <- predict(bigTree,validation,type = "class")

fittedLRTree <- predict(lrTree,training,type = "prob")
fittedBigTree <- predict(bigTree,training,type = "prob")
```

## Subset Model
```{r}
lrAccuracy <- (length((which(predLRTree == validation$INS))) / nrow(validation))

lrAccuracy
```


## Full Variable Model
```{r}
bigAccuracy <- (length((which(predBigTree == validation$INS))) / nrow(validation))

bigAccuracy
```


## Logistic Regression Accuracy
```{r}
# Taken from logistic regression ROC curve
cutoff <-  0.2970672
pred <- predict(finalModel,validationBin,type = "response")
pred <- data.frame(pred = pred) %>% mutate(pred = if_else(pred > cutoff,1,0))
pred <- pred$pred

# Create accuracy vector
accDF <- data.frame(pred = pred, observed = validation$INS) %>% mutate(accuracy = if_else(pred == observed,1,0))

accuracy <- round(mean(accDF$accuracy),4)
# Accuracy 
accuracy
```

Seeing as how the full variable tree outperforms both the logistic regression, and 